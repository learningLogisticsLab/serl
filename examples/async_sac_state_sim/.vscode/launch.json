{
    // VSCode debug configuration version
    "version": "0.2.0",
    "configurations": [
        // LEARNER
        {
            // Configuration for the RL agent learner component
            "name": "Python: Learner",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/async_sac_state_sim.py",
            // Command-line arguments matching run_learner.sh
            "args": [
                "--learner",                          // REQUIRED: Indicates this is a learner instance
                "--env", "PandaReachCube-v0",         // Environment to use
                "--exp_name=SERL-ReachTask",          // Experiment name for wandb logging
                "--seed", "0",                        // Random seed for reproducibility
                "--max_steps", "100000",              // Maximum training steps
                "--training_starts", "1000",          // Start training after buffer has this many samples
                "--critic_actor_ratio", "8",          // Critic-to-actor update ratio
                "--batch_size", "256",                // Training batch size
                "--replay_buffer_capacity", "100000", // Replay buffer capacity
            ],
            "console": "integratedTerminal",          // Use integrated terminal to see output
            "justMyCode": false,                      // Allow stepping into libraries
            // Environment variables from run_learner.sh
            "env": {
                "XLA_PYTHON_CLIENT_PREALLOCATE": "false",  // Don't preallocate JAX/XLA memory
                "XLA_PYTHON_CLIENT_MEM_FRACTION": ".5"    // Limit JAX memory usage to 50%
            },
            // Additional helpful debugging options
            "showReturnValue": true,                  // Show function return values
            "purpose": ["debug-in-terminal"],         // Run in terminal for better output
            "cwd": "${workspaceFolder}"               // Set working directory explicitly
        },
        // ACTOR
        {
            // Configuration for the RL agent actor component
            "name": "Python: Actor",
            "type": "debugpy",
            "request": "launch",
            "program": "${workspaceFolder}/async_sac_state_sim.py",
            // Command-line arguments matching run_actor.sh
            "args": [
                "--actor",                            // REQUIRED: Indicates this is an actor instance
                "--env", "PandaReachCube-v0",         // Environment to use
                "--exp_name=SERL-ReachTask",              // Experiment name for wandb logging
                "--seed", "0",                        // Random seed for reproducibility
                "--random_steps", "1000",             // Number of random steps at beginning
                "--max_steps", "100000",              // Maximum training steps
                "--training_starts", "1000",          // Start training after buffer has this many samples
                "--critic_actor_ratio", "8",          // Critic-to-actor update ratio
                "--batch_size", "256",                // Training batch size
                "--replay_buffer_capacity", "100000", // Replay buffer capacity
            ],
            "console": "integratedTerminal",          // Use integrated terminal to see output
            "justMyCode": false,                      // Allow stepping into libraries
            // Environment variables from run_actor.sh
            "env": {
                "XLA_PYTHON_CLIENT_PREALLOCATE": "false",  // Don't preallocate JAX/XLA memory
                "XLA_PYTHON_CLIENT_MEM_FRACTION": ".5"    // Limit JAX memory usage to 50%
            },
            "showReturnValue": true,                  // Show function return values
            "purpose": ["debug-in-terminal"],         // Run in terminal for better output
            "cwd": "${workspaceFolder}"               // Set working directory explicitly
        }
    ],
    // Compound configurations to launch multiple configurations together
    "compounds": [
        {
            // Launch both learner and actor at the same time
            "name": "Learner + Actor",
            "configurations": ["Python: Learner", "Python: Actor"],
            // Note: Actor will connect to learner via TrainerClient, assuming localhost IP
            // Both will run in separate debug sessions with independent controls
        }
    ],
    
    /*
    DEBUGGING TIPS:
    
    - IMPORTANT: You must select either Learner or Actor configuration when debugging
      (the NotImplementedError occurs if neither --learner nor --actor flag is specified)
    
    - If you get an error about 'utd_ratio', use the "Python: Learner (Fix utd_ratio)" configuration
      which adds this missing parameter
    
    - Set breakpoints in learner() or actor() functions to step through the main training loops
    
    - Key places to set breakpoints:
      * In actor(): near the action sampling logic (step < FLAGS.random_steps)
      * In learner(): where agent.update_high_utd() is called
      * Server/client communication points (client.update(), server.publish_network())
    
    - For memory issues: Watch replay buffer size growth with breakpoints in data_store.insert()
    
    - JAX issues: Set breakpoints after jax.device_put() calls to ensure proper device placement
    
    - The "utd_ratio" parameter seems to be used in the learner function but isn't defined 
      in the FLAGS. Use the special configuration or add a --utd_ratio flag to fix.
    
    DEBUG WORKFLOW:
    
    1. Start with "Python: Learner (Fix utd_ratio)" configuration
    2. Set breakpoints at key sections you want to monitor
    3. Run the debugger and observe variable values at each step
    4. Once learner is properly running, launch the Actor in a separate instance
    5. Watch for communication between the two
    */
}